{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVP Guide: Training Arm Grip and Navigation for PuzzArm (Focus on One Piece)\n",
    "\n",
    "\n",
    "The PuzzArm Minimum Viable Product (MVP) focuses on training a single xArm1S robotic arm on a Jetson Nano to autonomously pick and place one puzzle piece (e.g., number \"3\") into its corresponding slot, handling arbitrary rotations.\n",
    "\n",
    "Using a pre-trained YOLO classifier for piece detection, the MVP uses imitation learning, inspired by NVIDIA's road-following approach, to **train two models**: one for gripping (adjusting from a \"ready-to-grab\" state above a fixed piece position) and another for navigation (moving from a \"ready-to-move\" lifted state to the piece's slot). \n",
    "Three pre-programmed states—**home**, **ready-to-grab**, and **ready-to-move**—simplify kinematics. \n",
    "\n",
    "For each model, **20-30** manual demonstrations (images) are collected by positioning the arm by hand, capturing top-down camera images and joint states (via serial feedback added to image file name). \n",
    "The grip model learns to align the end-effector based on piece rotation, using end-effector voltage feedback to confirm success. The navigation model then guides the arm to the slot. \n",
    "\n",
    "Training uses PyTorch on Jetson, with TensorRT export for real-time inference (~50ms). The MVP, achievable in 4-6 hours, targets 85%+ success rate for one piece, forming the foundation for scaling to full puzzle automation.\n",
    "\n",
    "\n",
    "This MVP targets:\n",
    "- **Grip Training**: 20-30 demos → Model to adjust from \"ready-to-grab\" to grip based on piece rotation.\n",
    "- **Navigation Training**: 20-30 demos → Model to move from \"ready-to-move\" to slot placement.\n",
    "- **Total Effort**: 4-6 hours (2 for data, 1-2 for training, 1 for testing).\n",
    "\n",
    "We'll modify NVIDIA's `road_following.ipynb` (from [NVIDIA-AI-IOT/jetbot](https://github.com/NVIDIA-AI-IOT/jetbot)) to output 6D joint deltas instead of steering/throttle. Train with PyTorch, export to TensorRT for ~50ms inference on Nano.\n",
    "We will use  https://github.com/ccourson/Hiwonder-xArm1S to control the arm.\n",
    "\n",
    "#### Step 1: Setup Pre-Programmed States\n",
    "Hardcode these as joint position arrays in your serial package. Use your Python wrapper to send them (e.g., `set_positions([j1,j2,...])`).\n",
    "\n",
    "**You will need to adapt the provided code as it is untested and should be considered sudo code !**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    },
    "ExecuteTime": {
     "end_time": "2025-10-17T02:47:10.112721Z",
     "start_time": "2025-10-17T02:47:10.101424Z"
    }
   },
   "source": [
    "# In your xArm serial module (adapt from docs)\n",
    "import xarm\n",
    "\n",
    "# arm is the first xArm detected which is connected to USB\n",
    "arm = xarm.Controller('USB')\n",
    "print('Battery voltage in volts:', arm.getBatteryVoltage())\n",
    "\n",
    "def set_state(state_name):\n",
    "    states = {\n",
    "        'home': [90, 90, 90, 90, 90, 90],  # Central safe pos (calibrate once)\n",
    "        'ready_to_grab': [45, 120, 90, 90, 135, 90],  # Above fixed piece spot, gripper open/down\n",
    "        'ready_to_move': [45, 120, 135, 90, 90, 90]   # Lifted 5-10cm, gripper closed\n",
    "    }\n",
    "    if state_name in states:\n",
    "        ser.write_positions(states[state_name])  # Your method: send via TTL serial\n",
    "        print(f\"Set to {state_name}\")\n",
    "    else:\n",
    "        print(\"Invalid state\")\n",
    "\n",
    "def get_joints():\n",
    "    return ser.read_positions()  # Returns list of 6 floats + voltages\n",
    "\n",
    "def check_grip_success(threshold=0.5):  # Via end-effector feedback\n",
    "    positions, voltages = ser.read_positions_and_voltages()\n",
    "    # Simple: Grip success if voltage spike on gripper servo (j6?)\n",
    "    return any(v > threshold for v in voltages[-1:])  # Tune threshold"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Battery voltage in volts: 7.536\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Calibrate**: Manually jog arm (via python ) to positions, record joints with `get_joints()`, hardcode. Test: `set_state('home')`.\n",
    "\n",
    "#### Step 2: Data Collection for Grip Training\n",
    "- **Process**: \n",
    "  1. Set to `ready_to_grab`.\n",
    "  2. Manually nudge arm over piece (by hand—gently push links; no teleop needed).\n",
    "  3. At good grip alignment (visually: end-effector over peg, aligned to rotation), capture:\n",
    "     - Cropped image (from YOLO bbox, top-down view).\n",
    "     - Current joints (`get_joints()`).\n",
    "     - Label: \"grip_action\" (model learns to output small deltas from ready_pos).\n",
    "  4. Repeat 20-30x, varying rotations (place piece rotated 0-360° in fixed spot).\n",
    "- **Modified Notebook**: Fork `road_following.ipynb` (download from [jetbot repo](https://github.com/NVIDIA-AI-IOT/jetbot/blob/master/examples/road_following.ipynb)). Changes:\n",
    "  - Input: Cropped image (224x224) + current joints (6D vector).\n",
    "  - Output: 6D joint deltas (e.g., [-2, 1, 0, 0, -1, 0] for fine tweaks).\n",
    "  - Data: Save as `dataset_grip/` with images + JSON (joints, deltas= target - current).\n",
    "\n",
    "Run this cell-by-cell in Jupyter on Jetson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Serial Setup (run once)\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "from jetbot import Camera  # Or your CSI cam\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from serial import *  # Your xArm serial\n",
    "\n",
    "# Init cam & serial\n",
    "camera = Camera.instance()\n",
    "ser = Serial('/dev/ttyUSB0', 115200)  # Your init\n",
    "dataset_dir = 'dataset_grip'\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "counter = 0\n",
    "\n",
    "# Buttons for capture\n",
    "button_capture = widgets.Button(description=\"Capture Grip Demo\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def capture_grip(change):\n",
    "    global counter\n",
    "    # Get image & crop (assume YOLO bbox ready; for MVP, crop full view to piece area)\n",
    "    image = camera.value\n",
    "    cropped = image[100:200, 150:250]  # Tune to piece region; resize to 224x224\n",
    "    cv2.imwrite(f'{dataset_dir}/image_{counter}.jpg', cv2.cvtColor(cropped, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    # Get current joints\n",
    "    current_joints = get_joints()  # [j1..j6]\n",
    "    target_joints = current_joints  # For grip, delta=0 at perfect; but record as-is for learning\n",
    "    \n",
    "    data = {'joints': current_joints, 'deltas': [0]*6}  # Deltas from ready_to_grab\n",
    "    with open(f'{dataset_dir}/joints_{counter}.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    counter += 1\n",
    "    print(f\"Captured {counter}: Joints {current_joints}\")\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        display(image)  # Show full image\n",
    "\n",
    "button_capture.on_click(capture_grip)\n",
    "display(button_capture, output)\n",
    "\n",
    "# Usage: Set arm to ready_to_grab, position manually, click Capture. Do 20-30x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tips**: Place piece in fixed spot (e.g., 10cm left of base). Rotate it each time. After collection: `ls dataset_grip/` should have ~25 image/JSON pairs.\n",
    "\n",
    "#### Step 3: Train Grip Model (TensorRT Export)\n",
    "- Adapt `train_resnet_regression.ipynb` from JetBot repo (or dusty-nv/jetson-inference  for TRT examples).\n",
    "- Model: CNN (ResNet18) on image → FC → 6D deltas. Input concat: flattened image + current joints.\n",
    "- Train: ~10-20 min on Nano\n",
    "-  **Train using Colab** ~1-2 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell: Training (run after data collection)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import glob\n",
    "\n",
    "class GripDataset(Dataset):\n",
    "    def __init__(self, dir_path):\n",
    "        self.images = sorted(glob.glob(f'{dir_path}/*.jpg'))\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self): return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transforms(img)\n",
    "        \n",
    "        json_path = img_path.replace('.jpg', '.json')\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        joints = torch.tensor(data['joints'], dtype=torch.float32)\n",
    "        deltas = torch.tensor(data['deltas'], dtype=torch.float32)  # Target adjustments\n",
    "        \n",
    "        return img, torch.cat([img.flatten(), joints]), deltas  # Flattened for simple FC\n",
    "\n",
    "dataset = GripDataset('dataset_grip')\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Model: Simple ResNet for regression\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(512 + 6, 128),  # +6 for joints\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 6)  # 6D deltas\n",
    ")\n",
    "model = model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(50):\n",
    "    for imgs, states, targets in dataloader:\n",
    "        preds = model(states.cuda())\n",
    "        loss = criterion(preds, targets.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "torch.save(model.state_dict(), 'grip_model.pth')\n",
    "\n",
    "# Export to ONNX for TensorRT (use trtexec or jetson-inference tools)\n",
    "torch.onnx.export(model, torch.randn(1, 512+6).cuda(), 'grip_model.onnx')\n",
    "# Then: trtexec --onnx=grip_model.onnx --saveEngine=grip_model.trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Test Grip**: Load model, from `ready_to_grab`: Detect piece rotation with YOLO, crop image, infer deltas, apply incrementally (e.g., 5 steps of small moves). Check `check_grip_success()`—if yes, `set_state('ready_to_move')`.\n",
    "\n",
    "#### Step 4: Data Collection & Training for Slot Navigation\n",
    "- **Process**: Mirror grip, but:\n",
    "  1. Grip piece (using new model), lift to `ready_to_move`.\n",
    "  2. Manually guide arm over target slot.\n",
    "  3. Capture: Cropped image (piece over slot) + joints.\n",
    "  4. 20-30x, varying slot approaches.\n",
    "- **Notebook**: Duplicate grip one, rename `dataset_nav/`. Deltas: From ready_move to placement pose.\n",
    "- **Training**: Same code, output 6D for navigation (longer sequences if needed via LSTM).\n",
    "\n",
    "#### Step 5: MVP Integration & Testing\n",
    "- **Full Loop** (in a ROS2 node or Jupyter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pseudo: Autonomous Grip + Nav\n",
    "set_state('ready_to_grab')\n",
    "image = camera.value  # Run YOLO: detect piece, get bbox/rotation\n",
    "cropped = crop_to_bbox(image, bbox)  # Your func\n",
    "state = torch.cat([torch.flatten(transforms.ToTensor()(cropped)), torch.tensor(get_joints())])\n",
    "with torch.no_grad():\n",
    "    deltas = model(state.unsqueeze(0).cuda())[0].cpu().numpy()\n",
    "# Apply deltas incrementally: for d in deltas: adjust_joints(d * 0.1)  # Smooth\n",
    "if check_grip_success():\n",
    "    set_state('ready_to_move')\n",
    "    # Repeat for nav model: infer to slot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Test**: 10 runs on one piece/slot. Success: Grip + place without collision. Debug: Log joints/images.\n",
    "- **Next**: Scale to all 10 pieces (reuse data), add MoveIt2 fallback.\n",
    "\n",
    "This gets your MVP gripping and placing one piece reliably. Fork the JetBot repo, add these cells—test on hardware. If serial errors, check Hiwonder repo . "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
